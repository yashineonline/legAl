# ğŸ“ Project 1: Multilingual Legal Summarizer (RAG App)
# Structure and Base Code (LangChain + GPT-4 + pgvector)

# Directory structure:
# multilingual-legal-rag/
# â”œâ”€â”€ app/
# â”‚   â”œâ”€â”€ main.py               # FastAPI backend
# â”‚   â”œâ”€â”€ rag_pipeline.py       # RAG pipeline logic
# â”‚   â”œâ”€â”€ summarizer.py         # GPT-4 summary module
# â”‚   â”œâ”€â”€ chunker.py            # PDF/text chunking module
# â”‚   â”œâ”€â”€ embeddings.py         # SentenceTransformer or OpenAI
# â”‚   â””â”€â”€ utils.py              # Misc helpers (lang detect, parsing)
# â”œâ”€â”€ frontend/
# â”‚   â”œâ”€â”€ streamlit_app.py      # Optional Streamlit UI
# â”œâ”€â”€ data/
# â”‚   â””â”€â”€ example_docs/         # Sample French/English legal PDFs
# â”œâ”€â”€ models/
# â”‚   â””â”€â”€ multilingual_model.py # Custom encoder for embedding
# â”œâ”€â”€ Dockerfile
# â”œâ”€â”€ requirements.txt
# â””â”€â”€ README.md

# Here is the backend entry point: main.py

from fastapi import FastAPI, UploadFile, File
from app.rag_pipeline import run_rag_pipeline
from app.utils import detect_language

app = FastAPI()

@app.post("/summarize")
async def summarize(file: UploadFile = File(...)):
    contents = await file.read()
    lang = detect_language(contents)
    summary = run_rag_pipeline(file.filename, contents, lang)
    return {"summary": summary, "language": lang}

# utils.py
from langdetect import detect

def detect_language(text_or_bytes):
    try:
        text = text_or_bytes.decode("utf-8") if isinstance(text_or_bytes, bytes) else text_or_bytes
        return detect(text)
    except:
        return "unknown"

# rag_pipeline.py
from app.chunker import chunk_document
from app.embeddings import embed_chunks, retrieve_relevant_chunks
from app.summarizer import generate_summary


def run_rag_pipeline(filename, raw_content, lang):
    chunks = chunk_document(raw_content, lang)
    embedded = embed_chunks(chunks)
    top_chunks = retrieve_relevant_chunks(embedded, lang)
    return generate_summary(top_chunks, lang)

# summarizer.py
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

llm = ChatOpenAI(model_name="gpt-4", temperature=0.3)

prompt_template = PromptTemplate(
    input_variables=["chunks"],
    template="""
    RÃ©sumez les informations suivantes en franÃ§ais juridique clair :
    {chunks}
    
    RÃ©sumÃ©:
    """
)

def generate_summary(chunks, lang):
    joined = "\n\n".join(chunks)
    prompt = prompt_template.format(chunks=joined)
    return llm.predict(prompt)

# requirements.txt (partial)
fastapi
langchain
openai
pgvector
sentence-transformers
python-magic
uvicorn
streamlit
langdetect
pdfplumber
